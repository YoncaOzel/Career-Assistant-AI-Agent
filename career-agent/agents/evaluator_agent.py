import json
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

SCORE_THRESHOLD = 7  # Re-generate if below this threshold


def evaluate_response(employer_message: str, agent_response: str) -> dict:
    """
    Evaluates the Career Agent's reply across 5 criteria.

    Criteria (0-2 points each):
        - professional_tone : Professional and polite language
        - clarity           : Clear and coherent content
        - completeness      : Does it fully answer the question?
        - safety            : No lies or hallucinations?
        - relevance         : Is it directly related to the employer's message?

    Args:
        employer_message: The original employer message
        agent_response  : The reply generated by the Career Agent

    Returns:
        dict: {
            "total_score"  : int,   # 0-10
            "scores"       : dict,  # per-criterion scores
            "feedback"     : str,   # why this score?
            "suggestions"  : str,   # how to improve?
            "approved"     : bool   # did it pass SCORE_THRESHOLD?
        }
    """
    eval_prompt = f"""
EVALUATE the following career assistant reply.

## Employer Message:
{employer_message}

## Assistant Reply:
{agent_response}

## Evaluation Criteria (0-2 points each):
- professional_tone : Is the language professional and polite?
- clarity           : Is the content clear and coherent?
- completeness      : Is the employer's question fully answered?
- safety            : Is there no false, fabricated, or harmful information?
- relevance         : Is the reply directly related to the incoming message?

Return only JSON, nothing else:
{{
    "professional_tone": <0-2>,
    "clarity": <0-2>,
    "completeness": <0-2>,
    "safety": <0-2>,
    "relevance": <0-2>,
    "feedback": "<briefly explain why you gave this total score>",
    "suggestions": "<if score is low, how to improve; if score is high, 'No changes needed'>"
}}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": eval_prompt}],
        temperature=0.3,
        response_format={"type": "json_object"},
    )

    result = json.loads(response.choices[0].message.content)

    scores = {
        "professional_tone": int(result.get("professional_tone", 0)),
        "clarity": int(result.get("clarity", 0)),
        "completeness": int(result.get("completeness", 0)),
        "safety": int(result.get("safety", 0)),
        "relevance": int(result.get("relevance", 0)),
    }
    total = sum(scores.values())

    return {
        "total_score": total,
        "scores": scores,
        "feedback": str(result.get("feedback", "")),
        "suggestions": str(result.get("suggestions", "")),
        "approved": total >= SCORE_THRESHOLD,
    }
